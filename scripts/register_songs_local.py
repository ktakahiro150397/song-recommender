"""
Èü≥Â£∞„Éï„Ç°„Ç§„É´„Çí„Éô„ÇØ„Éà„É´DB„Å´ÁôªÈå≤„Åô„Çã„Çπ„ÇØ„É™„Éó„Éà

‰Ωø„ÅÑÊñπ:
    uv run register_songs.py                  # „Éê„ÉÉ„ÉÅÂá¶ÁêÜÔºà„Éá„Éï„Ç©„É´„ÉàÔºâ
    uv run register_songs.py --youtube-queue  # YouTube„Ç≠„É•„Éº„Åã„ÇâÂá¶ÁêÜ

„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Å´„Çà„Çä„ÄÅË§áÊï∞Êõ≤„Çí‰∏ÄÊã¨„ÅßDB„Å´ÁôªÈå≤„Åô„Çã„Åì„Å®„Åß„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„É™„ÇØ„Ç®„Çπ„ÉàÂõûÊï∞„ÇíÂâäÊ∏õ„Åó„ÄÅ
„É™„É¢„Éº„ÉàChromaDB„Å∏„ÅÆÁôªÈå≤ÈÄüÂ∫¶„ÇíÂ§ßÂπÖ„Å´ÊîπÂñÑ„Åó„Åæ„Åô„ÄÇ
"""

import argparse
import os
import re
import signal
import subprocess
import sys
import tempfile
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Optional

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from core.db_manager import SongVectorDB
from core.feature_extractor import FeatureExtractor
from core.song_queue_db import SongQueueDB
from core import song_metadata_db
from config import DB_CONFIGS

# „Ç∞„É≠„Éº„Éê„É´Â§âÊï∞Ôºö‰∏≠Êñ≠„Éï„É©„Ç∞
_interrupted = False
_processing_count = 0
_total_files = 0

# ========== ÂÆöÊï∞Ë®≠ÂÆö ==========

# ÁôªÈå≤ÂØæË±°„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™
SOUND_DIRS = [
    # "data/scsp_mv",
    # "data/gakumas_mv",
    r"F:\song-recommender-data\data\ALSTROEMERIA",
    # "F:/million",
]

# Èü≥Â£∞ÊäΩÂá∫Ë®≠ÂÆö
DURATION = 90  # Áßí

# „Éê„ÉÉ„ÉÅÂá¶ÁêÜË®≠ÂÆö
BATCH_SIZE = 500  # ‰∏ÄÂ∫¶„Å´ÁôªÈå≤„Åô„ÇãÊõ≤Êï∞ÔºàChromaDB„Å∏„ÅÆÊìç‰ΩúÂõûÊï∞„ÇíÂâäÊ∏õÔºâ

# „Çª„Ç∞„É°„É≥„ÉàÁôªÈå≤Ë®≠ÂÆö
SEGMENT_SECONDS = 5.0
SEGMENT_BATCH_SIZE = 16
SEGMENT_MODELS = [
    {
        "collection": "songs_segments_mert",
        "model": "m-a-p/MERT-v1-95M",
    },
    {
        "collection": "songs_segments_ast",
        "model": "MIT/ast-finetuned-audioset-10-10-0.4593",
    },
]


def clear_segment_search_cache() -> None:
    from core.database import get_session
    from sqlalchemy import text

    with get_session() as session:
        session.execute(
            text("DELETE FROM song_recommender.segment_search_cache WHERE id <> 0;")
        )
        session.commit()


# ========== „Ç∑„Ç∞„Éä„É´„Éè„É≥„Éâ„É© ==========


def signal_handler(sig, frame):
    """Ctrl+C (SIGINT) „ÇíÊçïÊçâ„Åó„Å¶ÂÆâÂÖ®„Å´ÁµÇ‰∫Ü„Åô„Çã"""
    global _interrupted
    if not _interrupted:
        _interrupted = True
        print("\n\n‚ö†Ô∏è  ‰∏≠Êñ≠„É™„ÇØ„Ç®„Çπ„Éà„ÇíÂèó‰ø°„Åó„Åæ„Åó„Åü...")
        print(
            f"   ÁèæÂú®Âá¶ÁêÜ‰∏≠„ÅÆ„Éï„Ç°„Ç§„É´„ÅåÂÆå‰∫Ü„Åó„Åü„ÇâÁµÇ‰∫Ü„Åó„Åæ„Åô ({_processing_count}/{_total_files})"
        )
        print("   „ÇÇ„ÅÜ‰∏ÄÂ∫¶ Ctrl+C „ÇíÊäº„Åô„Å®Âº∑Âà∂ÁµÇ‰∫Ü„Åó„Åæ„Åô\n")
    else:
        print("\nüõë Âº∑Âà∂ÁµÇ‰∫Ü„Åó„Åæ„Åô...")
        sys.exit(1)


# ========== „Éò„É´„Éë„ÉºÈñ¢Êï∞ ==========


def extract_youtube_id(filename: str) -> str | None:
    """
    „Éï„Ç°„Ç§„É´Âêç„Åã„ÇâYouTubeÂãïÁîªID„ÇíÊäΩÂá∫„Åô„Çã
    ‰æã: "Êõ≤Âêç [abcd1234XYZ].mp3" ‚Üí "abcd1234XYZ"
    """
    match = re.search(r"\[([a-zA-Z0-9_-]{11})\]", filename)
    return match.group(1) if match else None


def extract_song_title(filename: str) -> str:
    """
    „Éï„Ç°„Ç§„É´Âêç„Åã„ÇâÊõ≤Âêç„ÇíÊäΩÂá∫„Åô„Çã

    ÂÑ™ÂÖàÈ†Ü‰Ωç:
    1. „Äå„ÄçÔºà„Ç´„ÇÆÊã¨ÂºßÔºâ„ÅßÂõ≤„Çè„Çå„Å¶„ÅÑ„Çã ‚Üí ÊúÄÂàù„ÅÆ„Äå„Äç„ÅÆ‰∏≠Ë∫´
    2. „Äê„ÄëÔºà„Åô„Åø„Ç´„ÉÉ„Ç≥Ôºâ„Åå„ÅÇ„ÇãÂ†¥Âêà ‚Üí „Äê„Äë„ÅßÂõ≤„Çè„Çå„Å¶„ÅÑ„Å™„ÅÑÈÉ®ÂàÜ„ÇíÊäΩÂá∫
    3. ‰∏äË®ò„Å´Ë©≤ÂΩì„Åó„Å™„ÅÑ ‚Üí [videoId]„Å®Êã°ÂºµÂ≠ê„ÄÅ()„ÇíÈô§Âéª„Åó„ÅüÊñáÂ≠óÂàó

    ‰æã:
        'ÂàùÊòüÂ≠¶Âúí „ÄåStar-mine„ÄçOfficial Music Video [xxx].wav' ‚Üí 'Star-mine'
        '„ÄêÂ≠¶Âúí„Ç¢„Ç§„Éâ„É´„Éû„Çπ„Çø„Éº MV„ÄëÂÖâÊôØ„ÄêÂ≠¶„Éû„Çπ„Äë [xxx].wav' ‚Üí 'ÂÖâÊôØ'
        '„Äê„Ç∑„É£„Éã„ÇΩ„É≥„ÄëÁôΩÁÄ¨ Âí≤ËÄ∂„ÄåÂçÉÂ§ú„Ç¢„É™„Ç¢„Äç3DMV [xxx].wav' ‚Üí 'ÂçÉÂ§ú„Ç¢„É™„Ç¢'
        'traveling [abc123XYZ].wav' ‚Üí 'traveling'
    """
    # 1. „Äå„ÄçÔºà„Ç´„ÇÆÊã¨ÂºßÔºâ„ÇíÂÑ™ÂÖà„ÉÅ„Çß„ÉÉ„ÇØ
    kakko_match = re.search(r"„Äå(.+?)„Äç", filename)
    if kakko_match:
        return kakko_match.group(1).strip()

    # 2. „Äê„ÄëÔºà„Åô„Åø„Ç´„ÉÉ„Ç≥Ôºâ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ„Åù„ÅÆÂ§ñÂÅ¥„ÅÆÊñáÂ≠óÂàó„ÇíÊäΩÂá∫
    if "„Äê" in filename and "„Äë" in filename:
        # [videoId] „Å®Êã°ÂºµÂ≠ê„ÇíÂÖà„Å´Èô§Âéª
        temp = re.sub(r"\s*\[[a-zA-Z0-9_-]{11}\]\.(wav|mp3)$", "", filename)
        # „Äê...„Äë„ÇíÈô§Âéª
        temp = re.sub(r"„Äê[^„Äë]*„Äë", "", temp)
        # ()Ôºà‰∏∏Êã¨ÂºßÔºâ„Å®ÔºàÔºâÔºàÂÖ®Ëßí‰∏∏Êã¨ÂºßÔºâ„ÇíÈô§Âéª
        temp = re.sub(r"[\(Ôºà][^\)Ôºâ]*[\)Ôºâ]", "", temp)
        # ‰ΩôÂàÜ„Å™Á©∫ÁôΩ„ÇíÊï¥ÁêÜ
        temp = re.sub(r"\s+", " ", temp).strip()
        if temp:
            return temp

    # 3. ÂæìÊù•„ÅÆ„É≠„Ç∏„ÉÉ„ÇØ: [videoId] „Å®Êã°ÂºµÂ≠ê„ÇíÈô§Âéª
    # [videoId].ext „Éë„Çø„Éº„É≥„ÇíÈô§Âéª
    temp = re.sub(r"\s*\[[a-zA-Z0-9_-]{11}\]\.(wav|mp3)$", "", filename)
    # [videoId] „ÅÆ„ÅøÔºàÊã°ÂºµÂ≠ê„Å™„ÅóÔºâ„ÅÆ„Éë„Çø„Éº„É≥„ÇÇÈô§Âéª
    temp = re.sub(r"\s*\[[a-zA-Z0-9_-]{11}\]$", "", temp)
    # Êã°ÂºµÂ≠ê„ÅÆ„Åø„ÅÆÂ†¥Âêà„ÇÇÈô§Âéª
    temp = re.sub(r"\.(wav|mp3)$", "", temp)
    # ()Ôºà‰∏∏Êã¨ÂºßÔºâ„Å®ÔºàÔºâÔºàÂÖ®Ëßí‰∏∏Êã¨ÂºßÔºâ„ÇíÈô§Âéª
    temp = re.sub(r"[\(Ôºà][^\)Ôºâ]*[\)Ôºâ]", "", temp)
    # ‰ΩôÂàÜ„Å™Á©∫ÁôΩ„ÇíÊï¥ÁêÜ
    temp = re.sub(r"\s+", " ", temp).strip()

    return temp if temp else filename


def normalize_data_path(path: str) -> str | None:
    """
    „Éë„Çπ„ÇíÊ≠£Ë¶èÂåñ„Åó„ÄÅdata/ÈÖç‰∏ã„ÅÆÁõ∏ÂØæ„Éë„Çπ„ÇíËøî„ÅôÔºàdata/„ÅØÈô§„ÅèÔºâ
    data/ÈÖç‰∏ã„Åß„Å™„ÅÑÂ†¥Âêà„ÅØNone„ÇíËøî„Åô

    ‰æã:
        "data/utada" ‚Üí "utada"
        "F:/xxx/data/million" ‚Üí "million"
        "F:/xxx/data/gakumas_mv/sub" ‚Üí "gakumas_mv/sub"
        "F:/million" ‚Üí NoneÔºàdata/ÈÖç‰∏ã„Åß„ÅØ„Å™„ÅÑÔºâ
    """
    # „Éë„ÇπÂå∫Âàá„Çä„ÇíÁµ±‰∏ÄÔºà/ „Å´Ôºâ
    normalized = path.replace("\\", "/")

    # "data/" „ÇíÂê´„ÇÄ„Åã„ÉÅ„Çß„ÉÉ„ÇØ
    data_idx = normalized.find("data/")
    if data_idx == -1:
        # data/xxx ÂΩ¢Âºè„Åß„ÅØ„Å™„ÅÑÔºà‰æã: "F:/million"Ôºâ
        return None

    # data/ ‰ª•Èôç„ÇíÊäΩÂá∫„Åó„ÄÅdata/ Ëá™‰Ωì„ÅØÈô§„Åè
    relative_path = normalized[data_idx + 5 :]  # "data/" „ÅÆ5ÊñáÂ≠óÂàÜ„Çí„Çπ„Ç≠„ÉÉ„Éó
    return relative_path if relative_path else None


def get_audio_files_recursive(base_dir: str) -> list[tuple[str, str, str]]:
    """
    ÊåáÂÆö„Éá„Ç£„É¨„ÇØ„Éà„É™ÈÖç‰∏ã„ÅÆÈü≥Â£∞„Éï„Ç°„Ç§„É´„ÇíÂÜçÂ∏∞ÁöÑ„Å´ÂèñÂæó„Åô„Çã

    Args:
        base_dir: „Éô„Éº„Çπ„Éá„Ç£„É¨„ÇØ„Éà„É™Ôºà‰æã: "F:/song-recommender-data/data"Ôºâ

    Returns:
        (ÂÆüÈöõ„ÅÆ„Éï„Ç°„Ç§„É´„Éë„Çπ, „Éï„Ç°„Ç§„É´Âêç, Ê≠£Ë¶èÂåñ„Åï„Çå„Åü„Éá„Ç£„É¨„ÇØ„Éà„É™) „ÅÆ„É™„Çπ„Éà
    """
    results = []

    for root, dirs, files in os.walk(base_dir):
        # Èü≥Â£∞„Éï„Ç°„Ç§„É´„Çí„Éï„Ç£„É´„Çø
        audio_files = [f for f in files if f.endswith((".wav", ".mp3"))]

        if not audio_files:
            continue

        # „Åì„ÅÆ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆÊ≠£Ë¶èÂåñ„Éë„Çπ
        normalized_dir = normalize_data_path(root)
        if normalized_dir is None:
            continue

        for filename in audio_files:
            file_path = os.path.join(root, filename)
            results.append((file_path, filename, normalized_dir))

    return results


def get_file_size_mb(file_path: str) -> float:
    """„Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫„ÇíMBÂçò‰Ωç„ÅßÂèñÂæó"""
    try:
        size_bytes = os.path.getsize(file_path)
        return round(size_bytes / (1024 * 1024), 2)
    except OSError:
        return 0.0


@dataclass
class SegmentModel:
    collection: str
    model_id: str
    db: SongVectorDB
    feature_extractor: object
    model: object
    target_sr: int
    device: object


def _load_segment_packages():
    try:
        import torch
        import torch.nn.functional as F
        import torchaudio
        from transformers import AutoFeatureExtractor, AutoModel
    except Exception as exc:
        raise RuntimeError(
            "Segment registration requires torch, torchaudio, and transformers. "
            "Install CUDA-enabled torch/torchaudio and transformers."
        ) from exc

    return torch, F, torchaudio, AutoFeatureExtractor, AutoModel


def _load_audio_mono(path: Path, target_sr: int) -> "torch.Tensor":
    torch, _, torchaudio, _, _ = _load_segment_packages()
    try:
        waveform, sr = torchaudio.load(str(path))
        if waveform.ndim == 2 and waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        if sr != target_sr:
            waveform = torchaudio.functional.resample(waveform, sr, target_sr)
        return waveform.squeeze(0)
    except Exception:
        import librosa

        y, _ = librosa.load(str(path), sr=target_sr, mono=True)
        return torch.from_numpy(y)


def _split_segments(
    waveform: "torch.Tensor",
    segment_seconds: float,
    sr: int,
    min_samples: int = 1,
) -> list["torch.Tensor"]:
    segment_samples = int(round(segment_seconds * sr))
    if segment_samples <= 0:
        raise ValueError("segment_seconds is too small for the sampling rate")
    if min_samples < 1:
        min_samples = 1

    segments: list["torch.Tensor"] = []
    for start in range(0, waveform.numel(), segment_samples):
        end = min(start + segment_samples, waveform.numel())
        segment = waveform[start:end]
        if segment.numel() < min_samples:
            continue
        segments.append(segment)
    return segments


def _mean_pool(
    hidden: "torch.Tensor", attention_mask: Optional["torch.Tensor"]
) -> "torch.Tensor":
    if attention_mask is None:
        return hidden.mean(dim=1)
    mask = attention_mask.unsqueeze(-1).to(hidden.dtype)
    summed = (hidden * mask).sum(dim=1)
    denom = mask.sum(dim=1).clamp(min=1)
    return summed / denom


def _infer_segment_embeddings(
    model: object,
    feature_extractor: object,
    segments: list["torch.Tensor"],
    sr: int,
    device: "torch.device",
    batch_size: int,
) -> list[list[float]]:
    torch, F, _, _, _ = _load_segment_packages()
    embeddings: list[list[float]] = []
    model.eval()

    for i in range(0, len(segments), batch_size):
        batch = segments[i : i + batch_size]
        batch_np = [seg.cpu().numpy() for seg in batch]
        inputs = feature_extractor(
            batch_np, sampling_rate=sr, return_tensors="pt", padding=True
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)

        hidden = outputs.last_hidden_state
        attention_mask = inputs.get("attention_mask")
        if attention_mask is not None and attention_mask.shape[1] != hidden.shape[1]:
            attention_mask = None
        pooled = _mean_pool(hidden, attention_mask)
        pooled = F.normalize(pooled, p=2, dim=1)
        embeddings.extend(pooled.cpu().tolist())

    return embeddings


def _build_segment_id(filename: str, index: int) -> str:
    return f"{filename}::seg_{index:04d}"


def _get_chroma_max_batch_size(db: SongVectorDB, default: int = 5000) -> int:
    try:
        max_size = db.client.get_max_batch_size()
        if isinstance(max_size, int) and max_size > 0:
            return max_size
    except Exception:
        pass
    try:
        max_size = db.collection._client.get_max_batch_size()  # type: ignore[attr-defined]
        if isinstance(max_size, int) and max_size > 0:
            return max_size
    except Exception:
        pass
    return default


def _get_chroma_safe_batch_size(
    db: SongVectorDB, margin: float = 0.9, default: int = 5000
) -> int:
    max_size = _get_chroma_max_batch_size(db, default=default)
    safe_size = int(max_size * margin)
    return max(safe_size, 1)


def _add_segment_embeddings_to_db(
    db: SongVectorDB,
    segment_items: list[tuple[str, list[float], dict]],
    source_dir: str | None,
) -> int:
    if not segment_items:
        return 0

    ids = [item[0] for item in segment_items]
    embeddings = [item[1] for item in segment_items]
    metadatas = []
    for _, _, metadata in segment_items:
        base_metadata = {"excluded_from_search": False}
        if source_dir is not None:
            base_metadata["source_dir"] = source_dir
        base_metadata.update(metadata)
        metadatas.append(base_metadata)

    db.collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas)  # type: ignore
    return len(ids)


def init_segment_models(device_preference: str = "auto") -> list[SegmentModel]:
    torch, _, _, AutoFeatureExtractor, AutoModel = _load_segment_packages()

    if device_preference == "cuda" or (
        device_preference == "auto" and torch.cuda.is_available()
    ):
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    segment_models: list[SegmentModel] = []
    for config in SEGMENT_MODELS:
        feature_extractor = AutoFeatureExtractor.from_pretrained(
            config["model"], trust_remote_code=True
        )
        model = AutoModel.from_pretrained(config["model"], trust_remote_code=True).to(
            device
        )
        target_sr = int(getattr(feature_extractor, "sampling_rate", 16000))
        db = SongVectorDB(collection_name=config["collection"], distance_fn="cosine")
        segment_models.append(
            SegmentModel(
                collection=config["collection"],
                model_id=config["model"],
                db=db,
                feature_extractor=feature_extractor,
                model=model,
                target_sr=target_sr,
                device=device,
            )
        )

    return segment_models


def register_segments_for_file(
    file_path: str,
    filename: str,
    normalized_dir: str,
    segment_models: list[SegmentModel],
    segment_seconds: float = SEGMENT_SECONDS,
    processed_collections: set[str] | None = None,
    waveform_cache: dict[int, tuple[object, int]] | None = None,
) -> dict[str, list[tuple[str, list[float], dict]]] | None:
    """
    1Êõ≤ÂàÜ„ÅÆ„Çª„Ç∞„É°„É≥„Éà„ÇíÊ∫ñÂÇô„Åô„ÇãÔºà„Éê„ÉÉ„ÉÅÂá¶ÁêÜÁî®Ôºâ

    Args:
        waveform_cache: {target_sr: (waveform, sr)} „ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•ËæûÊõ∏Ôºà‰ªªÊÑèÔºâ

    Returns:
        {collection_name: [(segment_id, embedding, metadata), ...]} „ÅÆËæûÊõ∏
        „Åæ„Åü„ÅØÂá¶ÁêÜ‰∏çË¶Å„ÅÆÂ†¥Âêà„ÅØNone
    """
    if not segment_models:
        return None

    # „Éê„ÉÉ„ÉÅ„ÅßÂá¶ÁêÜÊ∏à„Åø„ÉÅ„Çß„ÉÉ„ÇØÔºàÂäπÁéáÂåñÔºâ
    if processed_collections is None:
        processed_collections = set(
            song_metadata_db.get_processed_collections(filename)
        )

    # ÂÖ®„É¢„Éá„É´„ÅßÂá¶ÁêÜÊ∏à„Åø„Å™„ÇâÊó©Êúü„É™„Çø„Éº„É≥
    all_processed = all(
        seg_model.collection in processed_collections for seg_model in segment_models
    )
    if all_processed:
        return None  # ‰Ωï„ÇÇÂá∫Âäõ„Åõ„ÅöÈùô„Åã„Å´„Çπ„Ç≠„ÉÉ„Éó

    # Â∞ë„Å™„Åè„Å®„ÇÇ1„Å§„ÅÆ„É¢„Éá„É´„ÅßÊú™Âá¶ÁêÜ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅÆ„Åø„Éï„Ç°„Ç§„É´„ÉÅ„Çß„ÉÉ„ÇØ
    audio_path = Path(file_path)
    if not audio_path.exists():
        print(f"   ‚ö†Ô∏è  „Çª„Ç∞„É°„É≥„ÉàÁôªÈå≤„Çπ„Ç≠„ÉÉ„Éó: {file_path} „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì")
        return None

    result: dict[str, list[tuple[str, list[float], dict]]] = {}

    for segment_model in segment_models:
        # MySQL„ÅßÂá¶ÁêÜÊ∏à„Åø„ÉÅ„Çß„ÉÉ„ÇØ
        if segment_model.collection in processed_collections:
            continue

        # „Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâÊ≥¢ÂΩ¢„Éá„Éº„Çø„ÇíÂèñÂæó„ÄÅ„Å™„Åë„Çå„Å∞„É≠„Éº„Éâ
        if waveform_cache is not None and segment_model.target_sr in waveform_cache:
            waveform = waveform_cache[segment_model.target_sr][0]
        else:
            waveform = _load_audio_mono(audio_path, segment_model.target_sr)
            # „Ç≠„É£„ÉÉ„Ç∑„É•„Å´‰øùÂ≠òÔºàÊ¨°„ÅÆ„É¢„Éá„É´„ÅßÂÜçÂà©Áî®Ôºâ
            if waveform_cache is not None:
                waveform_cache[segment_model.target_sr] = (
                    waveform,
                    segment_model.target_sr,
                )

        min_samples = 1
        win_length = getattr(segment_model.feature_extractor, "win_length", None)
        n_fft = getattr(segment_model.feature_extractor, "n_fft", None)
        if isinstance(win_length, int) and win_length > 1:
            min_samples = win_length
        elif isinstance(n_fft, int) and n_fft > 1:
            min_samples = n_fft

        segments = _split_segments(
            waveform,
            segment_seconds,
            segment_model.target_sr,
            min_samples=min_samples,
        )
        if not segments:
            print(f"   ‚ö†Ô∏è  „Çª„Ç∞„É°„É≥„Éà„ÅåÁîüÊàê„Åß„Åç„Åæ„Åõ„Çì: {filename}")
            continue

        embeddings = _infer_segment_embeddings(
            model=segment_model.model,
            feature_extractor=segment_model.feature_extractor,
            segments=segments,
            sr=segment_model.target_sr,
            device=segment_model.device,
            batch_size=SEGMENT_BATCH_SIZE,
        )

        total_duration = len(waveform) / segment_model.target_sr
        segment_items: list[tuple[str, list[float], dict]] = []
        for index, embedding in enumerate(embeddings):
            start_sec = index * segment_seconds
            end_sec = min(start_sec + segment_seconds, total_duration)
            metadata = {
                "segment_index": index,
                "segment_start_sec": round(start_sec, 3),
                "segment_end_sec": round(end_sec, 3),
                "segment_seconds": round(segment_seconds, 3),
                "source_song_id": filename,
                "source_path": str(audio_path),
                "model": segment_model.model_id,
            }
            segment_items.append(
                (_build_segment_id(filename, index), embedding, metadata)
            )

        result[segment_model.collection] = segment_items

    return result if result else None


def download_youtube_audio(video_id: str, output_dir: str) -> tuple[bool, str, str]:
    """
    yt-dlp„Çí‰ΩøÁî®„Åó„Å¶YouTubeÂãïÁîª„Åã„ÇâÈü≥Â£∞„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ

    Args:
        video_id: YouTubeÂãïÁîªID
        output_dir: Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™

    Returns:
        (ÊàêÂäü„Éï„É©„Ç∞, „É°„ÉÉ„Çª„Éº„Ç∏, „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åü„Éï„Ç°„Ç§„É´„Éë„Çπ)
    """
    try:
        # yt-dlp„Ç≥„Éû„É≥„Éâ„ÇíÊßãÁØâ
        output_template = os.path.join(output_dir, f"%(title)s [{video_id}].%(ext)s")
        cmd = [
            "yt-dlp",
            "-x",  # Èü≥Â£∞„ÅÆ„ÅøÊäΩÂá∫
            "--audio-format",
            "wav",  # WAVÂΩ¢Âºè„Åß‰øùÂ≠ò
            "--audio-quality",
            "0",  # ÊúÄÈ´òÂìÅË≥™
            "-o",
            output_template,
            f"https://www.youtube.com/watch?v={video_id}",
        ]

        # „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÂÆüË°å
        result = subprocess.run(
            cmd, capture_output=True, text=True, encoding="utf-8", timeout=300
        )

        if result.returncode != 0:
            return False, f"„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Ç®„É©„Éº: {result.stderr}", ""

        # „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„ÇíÊé¢„ÅôÔºà„Éñ„É©„Ç±„ÉÉ„Éà‰ªò„Åç„ÅÆvideo_id„ÇíÂê´„ÇÄ„Éï„Ç°„Ç§„É´Ôºâ
        downloaded_files = [
            f
            for f in Path(output_dir).glob("*")
            if f.is_file() and f"[{video_id}]" in f.name
        ]

        if not downloaded_files:
            return False, "„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åü„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì", ""

        file_path = str(downloaded_files[0])
        return True, "„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÊàêÂäü", file_path

    except subprocess.TimeoutExpired:
        return (
            False,
            "„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åå„Çø„Ç§„É†„Ç¢„Ç¶„Éà„Åó„Åæ„Åó„ÅüÔºà5ÂàÜ‰ª•ÂÜÖ„Å´ÂÆå‰∫Ü„Åó„Åæ„Åõ„Çì„Åß„Åó„ÅüÔºâ",
            "",
        )
    except FileNotFoundError:
        return (
            False,
            "yt-dlp„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
            "",
        )
    except Exception as e:
        return False, f"‰∫àÊúü„Åó„Å™„ÅÑ„Ç®„É©„Éº: {str(e)}", ""


def process_youtube_queue(parallel_mode: str = "none") -> None:
    """
    YouTube„Ç≠„É•„ÉºDB„Åã„ÇâÊú™Âá¶ÁêÜ„ÅÆÊõ≤„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„ÉªÁôªÈå≤„Åô„Çã

    Args:
        parallel_mode: ‰∏¶ÂàóÂá¶ÁêÜ„É¢„Éº„ÉâÔºànone/thread/processÔºâ
    """
    print("=" * 60)
    print("üéµ YouTube„Ç≠„É•„Éº„Åã„ÇâÈü≥Â£∞„Éï„Ç°„Ç§„É´„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„ÉªÁôªÈå≤")
    print(f"   ‰∏¶Âàó„É¢„Éº„Éâ: {parallel_mode}")
    print("=" * 60)

    # „Ç≠„É•„ÉºDB„ÇíÂàùÊúüÂåñ
    print("\nüîå YouTube„Ç≠„É•„ÉºDBÊé•Á∂ö‰∏≠...")
    try:
        queue_db = SongQueueDB()
        print("‚úÖ YouTube„Ç≠„É•„ÉºDBÊé•Á∂öÊàêÂäü")
    except Exception as e:
        print(f"‚ùå YouTube„Ç≠„É•„ÉºDBÊé•Á∂ö„Ç®„É©„Éº: {str(e)}")
        raise

    print("üìù Êú™Âá¶ÁêÜ„ÅÆÊõ≤„ÇíÂèñÂæó‰∏≠...")
    pending_songs = queue_db.get_pending_songs()
    print(f"‚úÖ ÂèñÂæóÂÆå‰∫Ü")

    if not pending_songs:
        print("\nÊú™Âá¶ÁêÜ„ÅÆÊõ≤„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì")
        return

    print(f"\nÊú™Âá¶ÁêÜ„ÅÆÊõ≤: {len(pending_songs)}‰ª∂\n")

    # „Éô„ÇØ„Éà„É´DB„ÇíÂàùÊúüÂåñ
    print("üìä „Éô„ÇØ„Éà„É´DB„ÇíÂàùÊúüÂåñ‰∏≠...")
    dbs_and_extractors = []
    for config in DB_CONFIGS:
        print(f"   üîå DBÊé•Á∂öÈñãÂßã: {config['collection']} (mode={config['mode']})")
        try:
            db = SongVectorDB(
                collection_name=config["collection"], distance_fn="cosine"
            )
            print(f"   ‚úÖ DBÊé•Á∂öÊàêÂäü: {config['collection']}")
            print(f"   üîß ÁâπÂæ¥ÈáèÊäΩÂá∫Âô®„ÇíÂàùÊúüÂåñ‰∏≠: mode={config['mode']}")
            extractor = FeatureExtractor(duration=DURATION, mode=config["mode"])
            print(f"   ‚úÖ ÁâπÂæ¥ÈáèÊäΩÂá∫Âô®ÂàùÊúüÂåñÂÆå‰∫Ü")
            dbs_and_extractors.append((db, extractor, config["mode"]))
            print(f"   üìä ÁèæÂú®„ÅÆDBÊõ≤Êï∞: {db.count()} Êõ≤\n")
        except Exception as e:
            print(f"   ‚ùå DBÂàùÊúüÂåñ„Ç®„É©„Éº: {config['collection']} - {str(e)}")
            raise

    print("üì¶ „Çª„Ç∞„É°„É≥„Éà„É¢„Éá„É´„ÇíÂàùÊúüÂåñ‰∏≠...")
    segment_models = init_segment_models()

    # ‰∏ÄÊôÇ„Éá„Ç£„É¨„ÇØ„Éà„É™„Çí‰ΩúÊàê
    temp_dir = tempfile.mkdtemp(prefix="youtube_audio_")
    print(f"\n   ‰∏ÄÊôÇ„Éá„Ç£„É¨„ÇØ„Éà„É™: {temp_dir}\n")

    success_count = 0
    failed_count = 0

    # „Ç∑„Ç∞„Éä„É´„Éè„É≥„Éâ„É©„ÇíË®≠ÂÆö
    global _interrupted, _processing_count, _total_files
    _interrupted = False
    _total_files = len(pending_songs)
    signal.signal(signal.SIGINT, signal_handler)

    try:
        for idx, song in enumerate(pending_songs, 1):
            # ‰∏≠Êñ≠„Éï„É©„Ç∞„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            if _interrupted:
                print("\n‚ö†Ô∏è  Âá¶ÁêÜ„Çí‰∏≠Êñ≠„Åó„Åæ„Åó„Åü")
                break

            _processing_count = idx
            video_id = song["video_id"]
            url = song["url"]

            print(f"[{idx}/{len(pending_songs)}] {video_id} - {url}")

            # ‚úÖ YouTubeID„ÅåDB„Å´Êó¢„Å´Â≠òÂú®„Åó„Å¶„ÅÑ„Çã„Åã„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            existing = song_metadata_db.get_by_youtube_id(video_id)
            youtube_id_exists = existing is not None
            if youtube_id_exists:
                print(f"   ‚è≠Ô∏è  YouTubeID ({video_id}) „ÅØÊó¢„Å´ÁôªÈå≤Ê∏à„Åø„Åß„Åô")
                print(f"      (Êó¢Â≠òID: {existing['song_id']})")

            if youtube_id_exists:
                queue_db.mark_as_processed(video_id)
                success_count += 1
                print()
                continue

            # „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ
            download_success, download_msg, file_path = download_youtube_audio(
                video_id, temp_dir
            )

            if not download_success:
                print(f"   ‚ùå „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÂ§±Êïó: {download_msg}")
                queue_db.mark_as_failed(video_id)
                failed_count += 1
                continue

            print(f"   ‚úÖ „ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÊàêÂäü: {os.path.basename(file_path)}")

            # „Éô„ÇØ„Éà„É´DB„Å´ÁôªÈå≤
            try:
                filename = os.path.basename(file_path)
                # song_queue„Å´‰øùÂ≠ò„Åï„Çå„Åü„É°„Çø„Éá„Éº„Çø„Çí‰ΩøÁî®
                normalized_dir = song.get("source_dir", "youtube")
                artist_name = song.get("artist_name")
                song_title = song.get("title")

                registered = False
                mysql_already_stored = False
                current_bpm = None
                for idx, (db, extractor, mode) in enumerate(dbs_and_extractors):
                    # ÂêÑDB„Å´ÂØæ„Åó„Å¶„ÄÅ„Åù„ÅÆDBÁî®„ÅÆÊäΩÂá∫Âô®„ÅßÁâπÂæ¥Èáè„ÇíÊäΩÂá∫
                    success, emb, bpm = add_song(
                        db,
                        extractor,
                        file_path,
                        filename,
                        normalized_dir,
                        embedding=None,  # ÂêÑDBÁôªÈå≤ÊôÇ„Å´Êñ∞„Åü„Å´ÊäΩÂá∫
                        artist_name=artist_name if not mysql_already_stored else None,
                        song_title_override=(
                            song_title if not mysql_already_stored else None
                        ),
                        skip_mysql=mysql_already_stored,
                        bpm=current_bpm,
                    )
                    if success:
                        registered = True
                        mysql_already_stored = True
                        if bpm is not None and current_bpm is None:
                            current_bpm = bpm

                # „Çª„Ç∞„É°„É≥„ÉàDBÁôªÈå≤
                try:
                    segment_result = register_segments_for_file(
                        file_path=file_path,
                        filename=filename,
                        normalized_dir=normalized_dir,
                        segment_models=segment_models,
                        segment_seconds=SEGMENT_SECONDS,
                    )

                    # YouTube„ÅÆÂ†¥Âêà„ÅØ1Êõ≤„Åö„Å§Âá¶ÁêÜ„Åô„Çã„ÅÆ„Åß„ÄÅÁµêÊûú„Åå„ÅÇ„Çå„Å∞„Åô„Åê„Å´ËøΩÂä†
                    if segment_result:
                        for segment_model in segment_models:
                            collection_name = segment_model.collection
                            segment_items = segment_result.get(collection_name, [])

                            if segment_items:
                                added = _add_segment_embeddings_to_db(
                                    db=segment_model.db,
                                    segment_items=segment_items,
                                    source_dir=normalized_dir,
                                )
                                if added > 0:
                                    song_metadata_db.mark_as_processed(
                                        filename, collection_name
                                    )
                                    print(
                                        f"   ‚úÖ {collection_name}: {added}„Çª„Ç∞„É°„É≥„ÉàÁôªÈå≤"
                                    )
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  „Çª„Ç∞„É°„É≥„ÉàÁôªÈå≤„Ç®„É©„Éº: {str(e)}")

                if registered:
                    print(f"   ‚úÖ DBÁôªÈå≤ÊàêÂäü (3DB„Å´ÁôªÈå≤)")
                    queue_db.mark_as_processed(video_id)
                    success_count += 1
                else:
                    print(f"   ‚ö†Ô∏è  Êó¢„Å´ÁôªÈå≤Ê∏à„Åø")
                    queue_db.mark_as_processed(video_id)
                    success_count += 1

            except Exception as e:
                print(f"   ‚ùå DBÁôªÈå≤Â§±Êïó: {str(e)}")
                queue_db.mark_as_failed(video_id)
                failed_count += 1

            # „ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„Åü„Éï„Ç°„Ç§„É´„ÇíÂâäÈô§
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
            except Exception as e:
                print(f"   ‚ö†Ô∏è  „Éï„Ç°„Ç§„É´ÂâäÈô§„Ç®„É©„Éº: {str(e)}")

            print()

    finally:
        # „Ç∑„Ç∞„Éä„É´„Éè„É≥„Éâ„É©„Çí„É™„Çª„ÉÉ„Éà
        signal.signal(signal.SIGINT, signal.SIG_DFL)

        # ‰∏ÄÊôÇ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÂâäÈô§
        try:
            import shutil

            shutil.rmtree(temp_dir, ignore_errors=True)
        except Exception as e:
            print(f"‚ö†Ô∏è  ‰∏ÄÊôÇ„Éá„Ç£„É¨„ÇØ„Éà„É™ÂâäÈô§„Ç®„É©„Éº: {str(e)}")

    print("\nüßπ „Çª„Ç∞„É°„É≥„ÉàÊ§úÁ¥¢„Ç≠„É£„ÉÉ„Ç∑„É•„Çí„ÇØ„É™„Ç¢‰∏≠...")
    try:
        clear_segment_search_cache()
        print("‚úÖ „Çª„Ç∞„É°„É≥„ÉàÊ§úÁ¥¢„Ç≠„É£„ÉÉ„Ç∑„É•„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åó„Åü")
    except Exception as e:
        print(f"‚ö†Ô∏è  „Ç≠„É£„ÉÉ„Ç∑„É•„ÇØ„É™„Ç¢Â§±Êïó: {str(e)}")

    # ÁµêÊûú„Çµ„Éû„É™„Éº
    print("=" * 60)
    print("üìä ÁµêÊûú„Çµ„Éû„É™„Éº")
    print("=" * 60)
    print(f"   ÊàêÂäü: {success_count} Êõ≤")
    print(f"   Â§±Êïó: {failed_count} Êõ≤")
    if _interrupted:
        print(f"   ‰∏≠Êñ≠: {len(pending_songs) - _processing_count} Êõ≤ÔºàÊú™Âá¶ÁêÜÔºâ")

    for db, _, mode in dbs_and_extractors:
        print(f"   DB ({mode}): {db.count()} Êõ≤")
    for segment_model in segment_models:
        print(f"   DB ({segment_model.collection}): {segment_model.db.count()} Êõ≤")

    if _interrupted:
        print("\n‚ö†Ô∏è  Âá¶ÁêÜ„Åå‰∏≠Êñ≠„Åï„Çå„Åæ„Åó„Åü")
    else:
        print("\n‚úÖ ÂÆå‰∫ÜÔºÅ")


# ========== „É°„Ç§„É≥Èñ¢Êï∞ ==========


def add_song(
    db: SongVectorDB,
    extractor: FeatureExtractor,
    file_path: str,
    filename: str,
    normalized_dir: str,
    embedding: list[float] | None = None,
    artist_name: str | None = None,
    song_title_override: str | None = None,
    skip_mysql: bool = False,
    bpm: float | None = None,
) -> tuple[bool, list[float] | None, float | None]:
    """
    1Êõ≤„ÇíDB„Å´ÁôªÈå≤„Åô„Çã

    Args:
        db: „Éô„ÇØ„Éà„É´DB
        extractor: ÁâπÂæ¥ÈáèÊäΩÂá∫Âô®
        file_path: ÂÆüÈöõ„ÅÆ„Éï„Ç°„Ç§„É´„Éë„Çπ
        filename: „Éï„Ç°„Ç§„É´Âêç
        normalized_dir: Ê≠£Ë¶èÂåñ„Åï„Çå„Åü„Éá„Ç£„É¨„ÇØ„Éà„É™„Éë„ÇπÔºàdata/„ÇíÈô§„ÅÑ„ÅüÂΩ¢ÂºèÔºâ
        embedding: ÁâπÂæ¥Èáè„Éô„ÇØ„Éà„É´ÔºàË§áÊï∞DBÁôªÈå≤ÊôÇ„ÅØ1Â∫¶„Å†„ÅëÊäΩÂá∫„Åô„Çã„Åü„ÇÅ„ÄÅ„ÅÆ‰∫åÂ∫¶ÁõÆ‰ª•Èôç„Å´ÊåáÂÆöÔºâ
        artist_name: „Ç¢„Éº„ÉÜ„Ç£„Çπ„ÉàÂêçÔºà‰ªªÊÑèÔºâ
        song_title_override: Êõ≤Âêç‰∏äÊõ∏„ÅçÔºà‰ªªÊÑèÔºâ
        skip_mysql: MySQL„Å∏„ÅÆÁôªÈå≤„Çí„Çπ„Ç≠„ÉÉ„Éó„Åô„Çã„ÅãÔºàË§áÊï∞DBÁôªÈå≤ÊôÇ„ÄÅ2Áï™ÁõÆ‰ª•Èôç„ÅÆDB„Åß„ÅØTrueÔºâ
        bpm: BPMÔºà„ÉÜ„É≥„ÉùÔºâ

    Returns:
        (ÁôªÈå≤„Åó„Åü„ÇâTrue„ÄÅembedding„Éô„ÇØ„Éà„É´„ÄÅBPM) „ÅÆ„Çø„Éó„É´
    """
    # ÂØæË±°„ÅÆÊã°ÂºµÂ≠ê„ÅÆ„ÅøÂá¶ÁêÜ
    if not (filename.endswith(".wav") or filename.endswith(".mp3")):
        return False, None, None

    collection_name = db.collection.name

    # „Çπ„ÉÜ„ÉÉ„Éó1: MySQL„É°„Çø„Éá„Éº„Çø„ÅÆÁ¢∫Ë™ç„Å®ÁôªÈå≤ÔºàÊúÄÂàù„ÅÆDBÁôªÈå≤ÊôÇ„ÅÆ„ÅøÔºâ
    metadata_exists = False
    if not skip_mysql:
        existing_song = song_metadata_db.get_song(song_id=filename)
        if existing_song is not None:
            metadata_exists = True
            # Êó¢Â≠ò„ÅÆBPM„ÇíÂèñÂæó
            if bpm is None and existing_song.get("bpm") is not None:
                bpm = existing_song["bpm"]
        else:
            # YouTube ID„Å´„Çà„ÇãÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØ
            youtube_id = extract_youtube_id(filename)
            if youtube_id:
                existing = song_metadata_db.get_by_youtube_id(youtube_id)
                if existing:
                    return False, None, None

            # „É°„Çø„Éá„Éº„Çø„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÁôªÈå≤
            # BPM„ÇíÊäΩÂá∫Ôºà„Åæ„Å†ÊäΩÂá∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥ÂêàÔºâ
            if bpm is None:
                try:
                    features = extractor.extract(file_path)
                    bpm = features.tempo
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  BPMÊäΩÂá∫„Ç®„É©„Éº ({filename}): {str(e)}")
                    bpm = None

            song_title = (
                song_title_override
                if song_title_override
                else extract_song_title(filename)
            )
            _, ext = os.path.splitext(filename)

            # artist_name„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅnormalized_dir„Çí‰ΩøÁî®
            if artist_name is None:
                artist_name = normalized_dir

            song_metadata_db.add_song(
                song_id=filename,
                filename=filename,
                song_title=song_title,
                artist_name=artist_name,
                source_dir=normalized_dir,
                youtube_id=youtube_id if youtube_id is not None else "",
                file_extension=ext.lower(),
                file_size_mb=get_file_size_mb(file_path),
                bpm=bpm,
                excluded_from_search=False,
            )
            metadata_exists = True
    else:
        metadata_exists = True  # skip_mysql„ÅÆÂ†¥Âêà„ÅØÊó¢„Å´Â≠òÂú®„Åô„Çã„Å®‰ªÆÂÆö

    # „Çπ„ÉÜ„ÉÉ„Éó2: „Éô„ÇØ„Éà„É´DB„ÅÆÂá¶ÁêÜÊ∏à„Åø„ÉÅ„Çß„ÉÉ„ÇØ
    if song_metadata_db.is_processed(song_id=filename, collection_name=collection_name):
        return False, None, bpm

    # „Çπ„ÉÜ„ÉÉ„Éó3: ÁâπÂæ¥ÈáèÊäΩÂá∫ÔºàÂøÖË¶Å„Å™Â†¥ÂêàÔºâ
    if embedding is None:
        try:
            embedding = extractor.extract_to_vector(file_path)
        except Exception as e:
            print(f"   ‚ùå ÁâπÂæ¥ÈáèÊäΩÂá∫„Ç®„É©„Éº ({filename}): {str(e)}")
            raise

    # „Çπ„ÉÜ„ÉÉ„Éó4: ChromaDB„Å∏„ÅÆ„Éô„ÇØ„Éà„É´ÁôªÈå≤
    db.add_song(
        song_id=filename,
        embedding=embedding,
        excluded_from_search=False,
        source_dir=normalized_dir,
    )

    # Âá¶ÁêÜÊ∏à„Åø„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„Å®„Åó„Å¶„Éû„Éº„ÇØ
    song_metadata_db.mark_as_processed(
        song_id=filename, collection_name=collection_name
    )

    return True, embedding, bpm


def prepare_song_data(
    extractor: FeatureExtractor,
    file_path: str,
    filename: str,
    normalized_dir: str,
    artist_name: str | None = None,
    song_title_override: str | None = None,
    waveform: tuple[object, int] | None = None,
) -> tuple[str, list[float], str, str, str, str, float, float | None] | None:
    """
    1Êõ≤ÂàÜ„ÅÆ„Éá„Éº„Çø„ÇíÊ∫ñÂÇô„Åô„ÇãÔºà„Éê„ÉÉ„ÉÅÂá¶ÁêÜÁî®Ôºâ

    Args:
        extractor: ÁâπÂæ¥ÈáèÊäΩÂá∫Âô®
        file_path: ÂÆüÈöõ„ÅÆ„Éï„Ç°„Ç§„É´„Éë„Çπ
        filename: „Éï„Ç°„Ç§„É´Âêç
        normalized_dir: Ê≠£Ë¶èÂåñ„Åï„Çå„Åü„Éá„Ç£„É¨„ÇØ„Éà„É™„Éë„Çπ
        artist_name: „Ç¢„Éº„ÉÜ„Ç£„Çπ„ÉàÂêçÔºà‰ªªÊÑèÔºâ
        song_title_override: Êõ≤Âêç‰∏äÊõ∏„ÅçÔºà‰ªªÊÑèÔºâ
        waveform: Êó¢„Å´„É≠„Éº„ÉâÊ∏à„Åø„ÅÆÊ≥¢ÂΩ¢„Éá„Éº„Çø (y, sr) „ÅÆ„Çø„Éó„É´Ôºà‰ªªÊÑèÔºâ

    Returns:
        (song_id, embedding, song_title, artist_name, youtube_id, file_extension, file_size_mb, bpm) „ÅÆ„Çø„Éó„É´„ÄÅ
        „Åæ„Åü„ÅØÂá¶ÁêÜ‰∏çË¶Å„ÅÆÂ†¥Âêà„ÅØNone
    """
    # ÂØæË±°„ÅÆÊã°ÂºµÂ≠ê„ÅÆ„ÅøÂá¶ÁêÜ
    if not (filename.endswith(".wav") or filename.endswith(".mp3")):
        return None

    # ÁâπÂæ¥ÈáèÊäΩÂá∫
    try:
        if waveform is not None:
            # Ê≥¢ÂΩ¢„Éá„Éº„Çø„Åã„ÇâÁõ¥Êé•ÊäΩÂá∫Ôºà„Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø„Å™„ÅóÔºâ
            y, sr = waveform
            features = extractor.extract_from_waveform(y, sr)
            embedding = features.to_vector(extractor.mode)
            bpm = features.tempo
        else:
            # „Éï„Ç°„Ç§„É´„Åã„ÇâË™≠„ÅøËæº„ÇÄ
            features = extractor.extract(file_path)
            embedding = features.to_vector(extractor.mode)
            bpm = features.tempo
    except Exception as e:
        print(f"   ‚ùå ÁâπÂæ¥ÈáèÊäΩÂá∫„Ç®„É©„Éº ({filename}): {str(e)}")
        return None

    # „É°„Çø„Éá„Éº„ÇøÊßãÁØâ
    youtube_id = extract_youtube_id(filename)
    song_title = (
        song_title_override if song_title_override else extract_song_title(filename)
    )
    _, ext = os.path.splitext(filename)

    # artist_name„ÅåÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅnormalized_dir„Çí‰ΩøÁî®
    if artist_name is None:
        artist_name = normalized_dir

    return (
        filename,  # song_id
        embedding,
        song_title,
        artist_name,
        youtube_id if youtube_id is not None else "",
        ext.lower(),
        get_file_size_mb(file_path),
        bpm,
    )


def add_songs_batch(
    db: SongVectorDB,
    song_data_list: list[
        tuple[str, list[float], str, str, str, str, float, float | None]
    ],
    normalized_dir: str,
    skip_mysql: bool = False,
) -> int:
    """
    Ë§áÊï∞„ÅÆÊõ≤„Çí‰∏ÄÊã¨„ÅßDB„Å´ÁôªÈå≤„Åô„ÇãÔºà„Éê„É´„ÇØ„Ç§„É≥„Çµ„Éº„ÉàÔºâ

    Args:
        db: „Éô„ÇØ„Éà„É´DB
        song_data_list: (song_id, embedding, song_title, artist_name, youtube_id, file_extension, file_size_mb, bpm) „ÅÆ„É™„Çπ„Éà
        normalized_dir: Ê≠£Ë¶èÂåñ„Åï„Çå„Åü„Éá„Ç£„É¨„ÇØ„Éà„É™„Éë„Çπ
        skip_mysql: MySQL„Å∏„ÅÆÁôªÈå≤„Çí„Çπ„Ç≠„ÉÉ„Éó„Åô„Çã„ÅãÔºàË§áÊï∞DBÁôªÈå≤ÊôÇ„ÄÅÊúÄÂàù„ÅÆDB‰ª•Â§ñ„Åß„ÅØTrueÔºâ

    Returns:
        ÁôªÈå≤„Åó„ÅüÊõ≤Êï∞
    """
    if not song_data_list:
        return 0

    song_ids = [data[0] for data in song_data_list]
    embeddings = [data[1] for data in song_data_list]
    collection_name = db.collection.name

    # MySQL„Å´„É°„Çø„Éá„Éº„Çø„Çí‰∏ÄÊã¨ÁôªÈå≤ÔºàÊúÄÂàù„ÅÆDBÁôªÈå≤„Åß„ÅÆ„ÅøÔºâ
    if not skip_mysql:
        from core.database import get_session
        from core.models import Song
        from sqlalchemy import delete

        # Êó¢Â≠ò„É¨„Ç≥„Éº„Éâ„ÇíÂÖà„Å´ÂâäÈô§ÔºàÈáçË§á„ÇíÈÅø„Åë„Çã„Åü„ÇÅÔºâ
        with get_session() as session:
            session.execute(delete(Song).where(Song.song_id.in_(song_ids)))
            session.commit()

        # Êñ∞„Åó„ÅÑ„É¨„Ç≥„Éº„Éâ„ÇíÊåøÂÖ•
        songs = []
        for data in song_data_list:
            (
                song_id,
                _,
                song_title,
                artist_name,
                youtube_id,
                file_extension,
                file_size_mb,
                bpm,
            ) = data
            songs.append(
                Song(
                    song_id=song_id,
                    filename=song_id,
                    song_title=song_title,
                    artist_name=artist_name,
                    source_dir=normalized_dir,
                    youtube_id=youtube_id,
                    file_extension=file_extension,
                    file_size_mb=file_size_mb,
                    bpm=bpm,
                    registered_at=datetime.now(),
                    excluded_from_search=False,
                )
            )

        with get_session() as session:
            session.bulk_save_objects(songs)

    # ProcessedCollection„ÅØÂêÑDBÁôªÈå≤ÊôÇ„Å´Ë®òÈå≤
    from core.database import get_session
    from core.models import ProcessedCollection

    processed_records = [
        ProcessedCollection(
            song_id=song_id,
            collection_name=collection_name,
            processed_at=datetime.now(),
        )
        for song_id in song_ids
    ]

    with get_session() as session:
        session.bulk_save_objects(processed_records)

    # ChromaDB„Å´„ÅØÊúÄÂ∞èÈôê„ÅÆ„Éá„Éº„Çø„ÅÆ„Åø‰øùÂ≠ò
    excluded_flags = [False] * len(song_ids)
    source_dirs = [normalized_dir] * len(song_ids)
    db.add_songs(song_ids, embeddings, excluded_flags, source_dirs)

    return len(song_data_list)


def process_single_db(args: tuple) -> bool:
    """
    ProcessPoolExecutorÁî®Ôºö1„Å§„ÅÆDB„Å´ÂØæ„Åó„Å¶ÁâπÂæ¥ÈáèÊäΩÂá∫ÔºÜÁôªÈå≤„ÇíË°å„ÅÜ
    Ôºà„Éó„É≠„Çª„ÇπÈñì„Åß„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÇíÊ∏°„Åõ„Å™„ÅÑ„Åü„ÇÅ„ÄÅÂºïÊï∞„Åã„ÇâÂÜçÊßãÁØâÔºâ

    Args:
        args: (db_config, file_path, filename, normalized_dir, duration)
    """
    db_config, file_path, filename, normalized_dir, duration = args

    # „Éó„É≠„Çª„ÇπÂÜÖ„ÅßDB„ÉªExtractor„ÇíÂàùÊúüÂåñ
    try:
        db = SongVectorDB(collection_name=db_config["collection"], distance_fn="cosine")
        extractor = FeatureExtractor(duration=duration, mode=db_config["mode"])
    except Exception as e:
        print(f"‚ùå „Éó„É≠„Çª„ÇπÂÜÖDBÂàùÊúüÂåñ„Ç®„É©„Éº ({db_config['collection']}): {str(e)}")
        raise

    return add_song(db, extractor, file_path, filename, normalized_dir)


def main():
    # ÂºïÊï∞„Éë„Éº„Çµ„Éº
    parser = argparse.ArgumentParser(description="Èü≥Â£∞„Éï„Ç°„Ç§„É´„Çí„Éô„ÇØ„Éà„É´DB„Å´ÁôªÈå≤")
    parser.add_argument(
        "--parallel",
        "-p",
        type=str,
        choices=["none", "thread", "process"],
        default="none",
        help="‰∏¶ÂàóÂá¶ÁêÜ„É¢„Éº„Éâ: none(Áõ¥Âàó), thread(ThreadPool), process(ProcessPool) - ÁèæÂú®„ÅØ‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„ÇìÔºàÂæåÊñπ‰∫íÊèõÊÄß„ÅÆ„Åü„ÇÅÊÆã„Åó„Å¶„ÅÑ„Åæ„ÅôÔºâ",
    )
    parser.add_argument(
        "--youtube-queue",
        "-y",
        action="store_true",
        help="YouTube„Ç≠„É•„ÉºDB„Åã„ÇâÊú™Âá¶ÁêÜ„ÅÆÊõ≤„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„ÉªÁôªÈå≤„Åô„Çã",
    )
    args = parser.parse_args()

    # YouTube„Ç≠„É•„Éº„É¢„Éº„Éâ„ÅÆÂ†¥Âêà
    if args.youtube_queue:
        process_youtube_queue(parallel_mode=args.parallel)
        return

    print("=" * 60)
    print("üéµ Èü≥Â£∞„Éï„Ç°„Ç§„É´„Çí„Éô„ÇØ„Éà„É´DB„Å´ÁôªÈå≤")

    chroma_probe = SongVectorDB(
        collection_name=DB_CONFIGS[0]["collection"], distance_fn="cosine"
    )
    chroma_max_batch = _get_chroma_max_batch_size(chroma_probe)
    chroma_safe_batch = _get_chroma_safe_batch_size(chroma_probe)
    if BATCH_SIZE > chroma_safe_batch:
        print(
            f"   ‚ö†Ô∏è  „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫‰∏äÈôê„ÇíÈÅ©Áî®: {BATCH_SIZE} ‚Üí {chroma_safe_batch}"
        )
    batch_size = min(BATCH_SIZE, chroma_safe_batch)

    print(f"   ChromaDB max batch size: {chroma_max_batch}")
    print(f"   „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫: {batch_size} Êõ≤/„Éê„ÉÉ„ÉÅÔºàÈü≥Â£∞„Éï„Ç°„Ç§„É´1ÂõûË™≠„ÅøËæº„ÅøÊúÄÈÅ©ÂåñÔºâ")
    print("=" * 60)

    # DB„ÉªÊäΩÂá∫Âô®„ÇíÂàùÊúüÂåñ
    print("\nüìä „Éô„ÇØ„Éà„É´DB„ÇíÂàùÊúüÂåñ‰∏≠...")
    dbs_and_extractors = []
    for config in DB_CONFIGS:
        print(f"   üîå DBÊé•Á∂öÈñãÂßã: {config['collection']} (mode={config['mode']})")
        try:
            db = SongVectorDB(
                collection_name=config["collection"], distance_fn="cosine"
            )
            print(f"   ‚úÖ DBÊé•Á∂öÊàêÂäü: {config['collection']}")
            print(f"   üîß ÁâπÂæ¥ÈáèÊäΩÂá∫Âô®„ÇíÂàùÊúüÂåñ‰∏≠: mode={config['mode']}")
            extractor = FeatureExtractor(duration=DURATION, mode=config["mode"])
            print(f"   ‚úÖ ÁâπÂæ¥ÈáèÊäΩÂá∫Âô®ÂàùÊúüÂåñÂÆå‰∫Ü")
            dbs_and_extractors.append((db, extractor, config["mode"]))
            print(f"   üìä ÁèæÂú®„ÅÆDBÊõ≤Êï∞: {db.count()} Êõ≤\n")
        except Exception as e:
            print(f"   ‚ùå DBÂàùÊúüÂåñ„Ç®„É©„Éº: {config['collection']} - {str(e)}")
            raise

    print("üì¶ „Çª„Ç∞„É°„É≥„Éà„É¢„Éá„É´„ÇíÂàùÊúüÂåñ‰∏≠...")
    segment_models = init_segment_models()

    print()

    total_added = 0
    total_skipped = 0

    # „Ç∑„Ç∞„Éä„É´„Éè„É≥„Éâ„É©„ÇíË®≠ÂÆö
    global _interrupted, _processing_count, _total_files
    _interrupted = False
    signal.signal(signal.SIGINT, signal_handler)

    try:
        for sound_dir in SOUND_DIRS:
            # data/ÈÖç‰∏ã„Åß„Å™„ÅÑ„Éë„Çπ„ÅØ„Çπ„Ç≠„ÉÉ„Éó
            if normalize_data_path(sound_dir) is None:
                print(f"‚ö†Ô∏è  Skipping {sound_dir}, not under data/ directory.")
                continue

            if not os.path.exists(sound_dir):
                print(f"‚ö†Ô∏è  Skipping {sound_dir}, directory not found.")
                continue

            print(f"\n--- Processing directory: {sound_dir} (recursive) ---")

            # ÂÜçÂ∏∞ÁöÑ„Å´Èü≥Â£∞„Éï„Ç°„Ç§„É´„ÇíÂèñÂæó
            audio_files = get_audio_files_recursive(sound_dir)
            print(f"    Found {len(audio_files)} audio files")

            current_dir = None
            _total_files = len(audio_files)

            # „Éê„ÉÉ„ÉÅÂá¶ÁêÜÁî®„ÅÆ‰∏ÄÊôÇ„É™„Çπ„Éà
            batch_files = []

            for idx, (file_path, filename, normalized_dir) in enumerate(audio_files, 1):
                # ‰∏≠Êñ≠„Éï„É©„Ç∞„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                if _interrupted:
                    print("\n‚ö†Ô∏è  Âá¶ÁêÜ„Çí‰∏≠Êñ≠„Åó„Åæ„Åó„Åü")
                    break

                _processing_count = idx

                # „Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂ§â„Çè„Å£„Åü„ÇâË°®Á§∫
                if normalized_dir != current_dir:
                    current_dir = normalized_dir
                    print(f"\n    üìÅ {normalized_dir}/")

                batch_files.append((file_path, filename, normalized_dir))

                # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Å´ÈÅî„Åó„Åü„Åã„ÄÅÊúÄÂæå„ÅÆ„Éï„Ç°„Ç§„É´„ÅÆÂ†¥Âêà„Å´Âá¶ÁêÜ
                if len(batch_files) >= batch_size or idx == len(audio_files):
                    # „Éê„ÉÉ„ÉÅÂÜÖ„ÅßÈáçË§á„Éï„Ç°„Ç§„É´Âêç„ÇíÈô§Â§ñÔºàÊúÄÂàù„ÅÆÂá∫Áèæ„ÅÆ„Åø‰øùÊåÅÔºâ
                    seen_filenames = set()
                    unique_batch_files = []
                    duplicate_count = 0
                    for file_path, filename, normalized_dir in batch_files:
                        if filename not in seen_filenames:
                            seen_filenames.add(filename)
                            unique_batch_files.append(
                                (file_path, filename, normalized_dir)
                            )
                        else:
                            duplicate_count += 1

                    if duplicate_count > 0:
                        print(
                            f"    ‚ÑπÔ∏è  „Éê„ÉÉ„ÉÅÂÜÖ„ÅÆÈáçË§á„Éï„Ç°„Ç§„É´„Çí„Çπ„Ç≠„ÉÉ„Éó: {duplicate_count} ‰ª∂"
                        )

                    batch_files = unique_batch_files

                    # „Éê„ÉÉ„ÉÅÂÜÖ„ÅÆ„Éï„Ç°„Ç§„É´Âêç„É™„Çπ„Éà„ÇíÂèñÂæó
                    batch_filenames = [f[1] for f in batch_files]

                    # Êó¢Â≠ò„ÉÅ„Çß„ÉÉ„ÇØÔºà„Éê„É´„ÇØ„ÇØ„Ç®„É™„ÄÅ„Ç®„É©„ÉºÊôÇ„ÅØ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
                    try:
                        existing_result = dbs_and_extractors[0][0].get_songs(
                            batch_filenames, include_embedding=False
                        )
                        existing_ids = set(existing_result.get("ids", []))
                    except Exception as e:
                        # ÈáçË§áID„Ç®„É©„Éº„ÇÑ„Åù„ÅÆ‰ªñ„ÅÆ„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„ÅüÂ†¥Âêà„ÅØÁ©∫„Å®„Åó„Å¶„Çπ„Ç≠„ÉÉ„Éó
                        print(f"    ‚ö†Ô∏è  Êó¢Â≠ò„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„ÉºÔºàÂá¶ÁêÜÁ∂ôÁ∂öÔºâ: {str(e)}")
                        existing_ids = set()

                    # Êú™ÁôªÈå≤„ÅÆ„Éï„Ç°„Ç§„É´„ÅÆ„ÅøÂá¶ÁêÜ
                    files_to_process = [
                        f for f in batch_files if f[1] not in existing_ids
                    ]

                    skipped_count = len(batch_files) - len(files_to_process)
                    total_skipped += skipped_count

                    if files_to_process:
                        print(
                            f"    „Éê„ÉÉ„ÉÅÂá¶ÁêÜ‰∏≠... ({len(files_to_process)} Êõ≤„ÄÅ{skipped_count} Êõ≤„Çπ„Ç≠„ÉÉ„Éó)"
                        )

                        # ÂÖ®DB„Å´ÂØæ„Åó„Å¶„ÄÅÂêÑDBÂ∞ÇÁî®„ÅÆ„É¢„Éº„Éâ„ÅßÁâπÂæ¥Èáè„ÇíÊäΩÂá∫„ÉªÁôªÈå≤
                        mysql_registered = False

                        # „Éê„ÉÉ„ÉÅÂÜÖ„ÅÆÂÖ®„Éï„Ç°„Ç§„É´„Çí‰∏ÄÂ∫¶„Å†„Åë„É≠„Éº„ÉâÔºà„É°„É¢„É™‰∏ä„ÅßÂÜçÂà©Áî®Ôºâ
                        waveform_cache: dict[str, tuple[object, int]] = (
                            {}
                        )  # {filename: (y, sr)}
                        load_errors = 0
                        for file_path, filename, normalized_dir in files_to_process:
                            if filename not in waveform_cache:
                                try:
                                    import librosa

                                    # ÊúÄÂàù„ÅÆextractor„ÅÆsr„Åß„É≠„Éº„Éâ
                                    y, sr = librosa.load(
                                        file_path,
                                        sr=dbs_and_extractors[0][1].sr,
                                        duration=dbs_and_extractors[0][1].duration,
                                    )
                                    waveform_cache[filename] = (y, sr)
                                except Exception as e:
                                    print(
                                        f"   ‚ùå Èü≥Â£∞„É≠„Éº„Éâ„Ç®„É©„Éº ({filename}): {str(e)}"
                                    )
                                    load_errors += 1

                        if waveform_cache:
                            success_msg = f"    üìÇ {len(waveform_cache)} Êõ≤„Çí„É°„É¢„É™„Å´„É≠„Éº„ÉâÊ∏à„ÅøÔºàÂÜçÂà©Áî®Ôºâ"
                            if load_errors > 0:
                                success_msg += f" ({load_errors} Êõ≤„É≠„Éº„ÉâÂ§±Êïó)"
                            print(success_msg)

                        for db, extractor, mode in dbs_and_extractors:
                            # ÂêÑDBÁî®„ÅÆÊäΩÂá∫Âô®„ÅßÁâπÂæ¥Èáè„ÇíÊäΩÂá∫ÔºàÊ≥¢ÂΩ¢„Éá„Éº„Çø„ÇíÂÜçÂà©Áî®Ôºâ
                            batch_data = []
                            current_normalized_dir = ""
                            for file_path, filename, normalized_dir in files_to_process:
                                current_normalized_dir = normalized_dir
                                waveform = waveform_cache.get(filename)
                                song_data = prepare_song_data(
                                    extractor,
                                    file_path,
                                    filename,
                                    normalized_dir,
                                    waveform=waveform,
                                )
                                if song_data:
                                    batch_data.append(song_data)

                            # MySQLÁôªÈå≤„ÅØÊúÄÂàù„ÅÆDBÁôªÈå≤„Åß„ÅÆ„Åø
                            skip_mysql = mysql_registered

                            if batch_data and current_normalized_dir:
                                try:
                                    added_count = add_songs_batch(
                                        db,
                                        batch_data,
                                        current_normalized_dir,
                                        skip_mysql=skip_mysql,
                                    )
                                    if not mysql_registered:  # ÊúÄÂàù„ÅÆDB„ÅÆ„Åø„Ç´„Ç¶„É≥„Éà
                                        total_added += added_count
                                        mysql_registered = True
                                    if added_count > 0:
                                        if mode != "minimal":
                                            print(
                                                f"    ‚úÖ {mode} DB „Å´ {added_count} Êõ≤ÁôªÈå≤"
                                            )
                                        else:
                                            print(f"    ‚úÖ {added_count} Êõ≤ÁôªÈå≤")
                                except Exception as e:
                                    print(
                                        f"    ‚ùå {mode} DB „Éê„ÉÉ„ÉÅÁôªÈå≤„Ç®„É©„Éº: {str(e)}"
                                    )
                    else:
                        pass
                        # print(f"    „Åô„Åπ„Å¶ÁôªÈå≤Ê∏à„Åø ({skipped_count} Êõ≤„Çπ„Ç≠„ÉÉ„Éó)")

                    # „Çª„Ç∞„É°„É≥„ÉàDBÁôªÈå≤Ôºà„Éê„ÉÉ„ÉÅ„ÉÅ„Çß„ÉÉ„ÇØ„ÅßÊúÄÈÅ©ÂåñÔºâ
                    # ÂÖ®„Å¶„ÅÆDB„Å´ÁôªÈå≤Ê∏à„Åø„Åã„Éê„ÉÉ„ÉÅ„ÉÅ„Çß„ÉÉ„ÇØ
                    all_collections = [
                        config["collection"] for config in DB_CONFIGS
                    ] + [model["collection"] for model in SEGMENT_MODELS]
                    batch_filenames_check = [f[1] for f in batch_files]

                    try:
                        processed_map = (
                            song_metadata_db.get_processed_collections_batch(
                                batch_filenames_check
                            )
                        )
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è  Âá¶ÁêÜÊ∏à„Åø„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„ÉºÔºàÂá¶ÁêÜÁ∂ôÁ∂öÔºâ: {str(e)}")
                        processed_map = {}

                    # „Éê„ÉÉ„ÉÅÂÜÖ„ÅÆÂÖ®Êõ≤ÂàÜ„ÅÆ„Çª„Ç∞„É°„É≥„Éà„ÇíÊ∫ñÂÇô
                    segments_to_add: dict[
                        str, list[tuple[str, list[float], dict, str]]
                    ] = {model["collection"]: [] for model in SEGMENT_MODELS}
                    songs_to_mark: dict[str, set[str]] = (
                        {}
                    )  # {filename: {collection1, collection2}}

                    # „Çª„Ç∞„É°„É≥„ÉàÁî®„ÅÆÊ≥¢ÂΩ¢„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÊ∫ñÂÇôÔºà„Éï„Ç°„Ç§„É´„Åî„Å®„Å´ÂêÑtarget_srÁî®„ÅÆÊ≥¢ÂΩ¢„Çí‰øùÊåÅÔºâ
                    segment_waveform_caches: dict[
                        str, dict[int, tuple[object, int]]
                    ] = {}

                    for file_path, filename, normalized_dir in batch_files:
                        # ÂÖ®„Å¶„ÅÆDB„Å´ÁôªÈå≤Ê∏à„Åø„Åã„ÉÅ„Çß„ÉÉ„ÇØÔºàÊó©Êúü„Çπ„Ç≠„ÉÉ„ÉóÔºâ
                        processed_collections = processed_map.get(filename, set())
                        if all(col in processed_collections for col in all_collections):
                            # ÂÖ®DBÁôªÈå≤Ê∏à„Åø - ÂÆåÂÖ®„Çπ„Ç≠„ÉÉ„Éó
                            continue

                        # „Åì„ÅÆ„Éï„Ç°„Ç§„É´Áî®„ÅÆÊ≥¢ÂΩ¢„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÊ∫ñÂÇôÔºàÂøÖË¶Å„Å´„Å™„Å£„Åü„Çâ„É≠„Éº„ÉâÔºâ
                        if filename not in segment_waveform_caches:
                            segment_waveform_caches[filename] = {}

                        try:
                            segment_result = register_segments_for_file(
                                file_path=file_path,
                                filename=filename,
                                normalized_dir=normalized_dir,
                                segment_models=segment_models,
                                segment_seconds=SEGMENT_SECONDS,
                                processed_collections=processed_collections,
                                waveform_cache=segment_waveform_caches[filename],
                            )

                            if segment_result:
                                for (
                                    collection_name,
                                    segment_items,
                                ) in segment_result.items():
                                    # normalized_dir„ÇíÂêÑ„Ç¢„Ç§„ÉÜ„É†„Å´ËøΩÂä†„Åó„Å¶„Éê„ÉÉ„Éï„Ç°„Å´ËìÑÁ©ç
                                    for seg_id, emb, meta in segment_items:
                                        segments_to_add[collection_name].append(
                                            (seg_id, emb, meta, normalized_dir)
                                        )

                                    # „Åì„ÅÆÊõ≤„Çí„Åì„ÅÆ„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„ÅßÂá¶ÁêÜÊ∏à„Åø„Å®„Åó„Å¶„Éû„Éº„ÇØ‰∫àÂÆö„Å´ËøΩÂä†
                                    if filename not in songs_to_mark:
                                        songs_to_mark[filename] = set()
                                    songs_to_mark[filename].add(collection_name)
                        except Exception as e:
                            print(f"    ‚ö†Ô∏è  „Çª„Ç∞„É°„É≥„ÉàÁôªÈå≤„Ç®„É©„Éº: {str(e)}")

                    # „Éê„ÉÉ„ÉÅÂÜÖ„ÅÆÂÖ®„Çª„Ç∞„É°„É≥„Éà„Çí‰∏ÄÊã¨„ÅßChromaDB„Å´ËøΩÂä†
                    for segment_model in segment_models:
                        collection_name = segment_model.collection
                        segment_batch = segments_to_add.get(collection_name, [])

                        if not segment_batch:
                            continue

                        # ChromaDB„Å∏„ÅÆ‰∏ÄÊã¨ËøΩÂä†
                        ids = [item[0] for item in segment_batch]
                        embeddings = [item[1] for item in segment_batch]
                        metadatas = []
                        for _, _, meta, source_dir in segment_batch:
                            full_meta = {
                                "excluded_from_search": False,
                                "source_dir": source_dir,
                            }
                            full_meta.update(meta)
                            metadatas.append(full_meta)

                        max_batch = _get_chroma_safe_batch_size(segment_model.db)
                        try:
                            for start in range(0, len(ids), max_batch):
                                end = start + max_batch
                                segment_model.db.collection.add(
                                    ids=ids[start:end],
                                    embeddings=embeddings[start:end],
                                    metadatas=metadatas[start:end],
                                )
                            print(
                                f"    ‚úÖ {collection_name}: {len(ids)}„Çª„Ç∞„É°„É≥„Éà‰∏ÄÊã¨ÁôªÈå≤"
                            )
                        except Exception as e:
                            print(f"    ‚ùå {collection_name} ‰∏ÄÊã¨ÁôªÈå≤„Ç®„É©„Éº: {str(e)}")
                            continue

                    # MySQL„Å´Âá¶ÁêÜÊ∏à„Åø„Éû„Éº„ÇØ„Çí‰∏ÄÊã¨ÁôªÈå≤
                    for filename, collections in songs_to_mark.items():
                        for collection_name in collections:
                            try:
                                song_metadata_db.mark_as_processed(
                                    filename, collection_name
                                )
                            except Exception as e:
                                print(
                                    f"    ‚ö†Ô∏è  Âá¶ÁêÜÊ∏à„Åø„Éû„Éº„ÇØÂ§±Êïó ({filename}, {collection_name}): {str(e)}"
                                )

                    # „Éê„ÉÉ„ÉÅ„Çí„ÇØ„É™„Ç¢
                    batch_files = []

    finally:
        # „Ç∑„Ç∞„Éä„É´„Éè„É≥„Éâ„É©„Çí„É™„Çª„ÉÉ„Éà
        signal.signal(signal.SIGINT, signal.SIG_DFL)

    print("\nüßπ „Çª„Ç∞„É°„É≥„ÉàÊ§úÁ¥¢„Ç≠„É£„ÉÉ„Ç∑„É•„Çí„ÇØ„É™„Ç¢‰∏≠...")
    try:
        clear_segment_search_cache()
        print("‚úÖ „Çª„Ç∞„É°„É≥„ÉàÊ§úÁ¥¢„Ç≠„É£„ÉÉ„Ç∑„É•„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åó„Åü")
    except Exception as e:
        print(f"‚ö†Ô∏è  „Ç≠„É£„ÉÉ„Ç∑„É•„ÇØ„É™„Ç¢Â§±Êïó: {str(e)}")

    # ÁµêÊûú„Çµ„Éû„É™„Éº
    print("\n" + "=" * 60)
    print("üìä ÁµêÊûú„Çµ„Éû„É™„Éº")
    print("=" * 60)
    print(f"   Êñ∞Ë¶èÁôªÈå≤: {total_added} Êõ≤")
    print(f"   „Çπ„Ç≠„ÉÉ„Éó: {total_skipped} Êõ≤ÔºàÁôªÈå≤Ê∏à„ÅøÔºâ")
    if _interrupted and _total_files > 0:
        print(f"   ‰∏≠Êñ≠: {_total_files - _processing_count} „Éï„Ç°„Ç§„É´ÔºàÊú™Âá¶ÁêÜÔºâ")

    for db, _, mode in dbs_and_extractors:
        print(f"   DB ({mode}): {db.count()} Êõ≤")
    for segment_model in segment_models:
        print(f"   DB ({segment_model.collection}): {segment_model.db.count()} Êõ≤")

    if _interrupted:
        print("\n‚ö†Ô∏è  Âá¶ÁêÜ„Åå‰∏≠Êñ≠„Åï„Çå„Åæ„Åó„Åü")
    else:
        print("\n‚úÖ ÂÆå‰∫ÜÔºÅ")


if __name__ == "__main__":
    main()
